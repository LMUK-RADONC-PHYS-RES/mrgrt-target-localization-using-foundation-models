{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "device = \"cuda\"  # cpu or cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation\n",
    "\n",
    "# this cell should be filled with your code for dataloading\n",
    "\n",
    "\n",
    "# As an example, we use a shor sequence of an circle oscillating in y direction\n",
    "from generate_example_sequence import generate_sequence\n",
    "sequence, path = generate_sequence(width=100, height=100, num_frames=100,\n",
    "                                   radius=0.3, amplitude=0.3, frequency=1, dtype=np.uint8)\n",
    "\n",
    "initial_mask = (sequence[0].copy() > 0).astype(np.uint8)\n",
    "\n",
    "# maybe add some (static) noise to the sequence\n",
    "noise = 0.0\n",
    "if noise > 0:\n",
    "    np.random.seed(0)\n",
    "    sequence = sequence + noise * np.random.uniform(0, 1, sequence[0].shape)\n",
    "    sequence = ((200/(1+noise))*sequence).astype(np.uint8)\n",
    "\n",
    "# convert the sequence to a rgb numpy array with\n",
    "sequence = np.repeat(sequence[..., None], 3, axis=3)\n",
    "\n",
    "\n",
    "# at this point sequence should be a numpy array of shape (T,H,W,C) where C=3 (rgb)\n",
    "assert sequence.shape[3] == 3\n",
    "assert sequence.dtype == np.uint8\n",
    "\n",
    "# and initial_mask should be a binary mask of shape (H,W) with values in {0,1}\n",
    "assert initial_mask.shape == sequence.shape[1:3]\n",
    "assert np.all(np.isin(np.unique(initial_mask), [0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, H, W, C = sequence.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ContourTracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the foundation models\n",
    "\n",
    "# cotracker2\n",
    "from cotracker.predictor import CoTrackerOnlinePredictor\n",
    "cotracker_predictor = CoTrackerOnlinePredictor(\"./checkpoints/cotracker2.pth\").to(device)\n",
    "\n",
    "# repvit_sam -> smaller and faster version of sam\n",
    "from repvitsam.predictor import SamPredictor\n",
    "from repvitsam.build_sam import build_sam_repvit\n",
    "\n",
    "sam_predictor = SamPredictor(build_sam_repvit(\"./checkpoints/repvit_sam.pt\").to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the contourtracker approach requires a few helper functions\n",
    "from contourtracker_utils import queries_from_points, \\\n",
    "    compute_logits_from_mask_torch, compute_sam_box, points_to_mask, \\\n",
    "    compute_largest_contour, interp_1d, compute_bounding_box, compute_dice_score_torch\n",
    "from cotracker.models.core.model_utils import get_points_on_a_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing cotracker queries\n",
    "\n",
    "def prepare_queries(mask,\n",
    "                    fixed_contour_count,\n",
    "                    device,\n",
    "                    bulk_grid_size,\n",
    "                    bulk_margin,\n",
    "                    cotracking_grid_size,\n",
    "                    original_size):\n",
    "\n",
    "    # queries_ is a list of cotracking points\n",
    "    # index is a dictionary that maps the query type to the start index and length of the queries in the queries list\n",
    "    queries_ = []\n",
    "    index = {}\n",
    "\n",
    "    contour = compute_largest_contour(mask)\n",
    "\n",
    "    # the contour is interpolated to fixed_contour_count points\n",
    "    # this is done to make the runtime of the model predictable.\n",
    "    contour = interp_1d(contour, fixed_contour_count)\n",
    "    queries = queries_from_points(contour, 0, device)\n",
    "\n",
    "    index[\"contour\"] = (0, queries.shape[1])\n",
    "    queries_.append(queries)\n",
    "\n",
    "    # local cotracking points\n",
    "    if bulk_grid_size > 0:\n",
    "\n",
    "        # grid_pts = get_points_on_a_grid(bulk_grid_size, mask.shape, device=frames.device)\n",
    "        m = bulk_margin  # 0.2\n",
    "        box = compute_bounding_box(mask)\n",
    "        # create a local grid of points around the bounding box\n",
    "        bulk_points = get_points_on_a_grid(\n",
    "            cotracking_grid_size,\n",
    "            ((1.0 + m)*(box[3] - box[1]), (1.0 + m)*(box[2] - box[0])),\n",
    "            device=device\n",
    "        )\n",
    "        # shift local grid to the position of the bounding box\n",
    "        bulk_points += torch.tensor([box[0], box[1]], device=device).float() - (\n",
    "            m/2) * torch.tensor([box[3] - box[1], box[2] - box[0]], device=device).float()\n",
    "\n",
    "        # clamp points to image boundaries\n",
    "        bulk_points[0, :, 0] = torch.clamp(\n",
    "            bulk_points[0, :, 0], 0, original_size[0]-1)\n",
    "        bulk_points[0, :, 1] = torch.clamp(\n",
    "            bulk_points[0, :, 1], 0, original_size[1]-1)\n",
    "\n",
    "        # add a t=0 channel to the points\n",
    "        bulk_queries = torch.cat(\n",
    "            [torch.zeros_like(bulk_points[:, :, :1]), bulk_points], dim=2)\n",
    "\n",
    "        index[\"local_cotracking_grid\"] = (\n",
    "            sum([q.shape[1] for q in queries_]), bulk_queries.shape[1])\n",
    "        queries_.append(bulk_queries)\n",
    "\n",
    "    # global cotracking points\n",
    "    if cotracking_grid_size > 0:\n",
    "        # create a global grid of points covering the whole image\n",
    "        grid_pts = get_points_on_a_grid(\n",
    "            cotracking_grid_size, original_size, device=device)\n",
    "\n",
    "        # add a t=0 channel to the points\n",
    "        grid_pts = torch.cat(\n",
    "            [torch.zeros_like(grid_pts[:, :, :1]), grid_pts], dim=2)\n",
    "\n",
    "        index[\"global_cotracking_grid\"] = (\n",
    "            sum([q.shape[1] for q in queries_]), grid_pts.shape[1])\n",
    "        queries_.append(grid_pts)\n",
    "\n",
    "    queries = torch.cat(queries_, dim=1)\n",
    "    return queries, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine mask with sam\n",
    "\n",
    "def refine_with_sam(cotracker_mask, sam_predictor, frame, original_size, sam_pass_box_margin=0.5, mask_threshold=0.0, embedding=None):\n",
    "\n",
    "    mask_prompt = compute_logits_from_mask_torch(\n",
    "        cotracker_mask.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    box = compute_sam_box(sam_predictor, cotracker_mask,\n",
    "                          original_size, margin=sam_pass_box_margin)\n",
    "    sam_prepred_frame = sam_predictor.transform.apply_image_torch(\n",
    "        frame.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "    chunk = [{\n",
    "        \"mask_inputs\": mask_prompt.unsqueeze(0),  # Bx1xHxW\n",
    "        \"boxes\": box.unsqueeze(0),  # Bx4\n",
    "        \"image\": sam_prepred_frame,\n",
    "        \"original_size\": original_size,\n",
    "        \"mask_threshold\": mask_threshold,\n",
    "        # **({\"image_embedding\":embedding} if embedding is not None else {})\n",
    "    }]\n",
    "    res = sam_predictor.model.forward(\n",
    "        chunk, multimask_output=True)  # B NumMasks H W\n",
    "\n",
    "    frame_seg = res[0]\n",
    "    # reshape\n",
    "    frame_seg[\"masks\"] = frame_seg[\"masks\"].reshape(1, -1, *original_size)\n",
    "    frame_seg[\"low_res_logits\"] = frame_seg[\"low_res_logits\"].reshape(\n",
    "        1, -1, *frame_seg[\"low_res_logits\"].shape[-2:])\n",
    "    frame_seg[\"iou_predictions\"] = frame_seg[\"iou_predictions\"].reshape(1, -1)\n",
    "\n",
    "    masks = []\n",
    "    iou_predictions = []\n",
    "    logits = []\n",
    "    # use the third sam mask -> which is the closest to the prompt\n",
    "    masks.append(frame_seg[\"masks\"][:, -2:-1])\n",
    "    logits.append(frame_seg[\"low_res_logits\"][:, -2:-1])\n",
    "    iou_predictions.append(frame_seg[\"iou_predictions\"])\n",
    "\n",
    "    masks = torch.stack(masks).squeeze(1, 2)\n",
    "    iou_predictions = torch.stack(iou_predictions).squeeze(1)\n",
    "    logits = torch.stack(logits).squeeze(1, 2)\n",
    "    return masks, iou_predictions, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_thresholding(initial_mask, logits, thresholds=np.linspace(-5.0, 5.0, 25)):\n",
    "    initial_mask_torch = torch.tensor(initial_mask, device=logits.device)\n",
    "\n",
    "    # interpolate the logits to the size of the initial mask\n",
    "    scaled_logits = torch.nn.functional.interpolate(\n",
    "        logits.unsqueeze(1),\n",
    "        initial_mask.shape,\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).squeeze(0, 1)\n",
    "\n",
    "    # find the best threshold with a basic grid search\n",
    "    best_threshold = 0.0\n",
    "    best_dice = 0.0\n",
    "    for threshold in thresholds:\n",
    "        d_score = compute_dice_score_torch(\n",
    "            scaled_logits > threshold, initial_mask_torch)\n",
    "        if d_score > best_dice:\n",
    "            best_dice = d_score\n",
    "            best_threshold = threshold\n",
    "    # set mask threshold to the best threshold\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the sequence to a tensor with shape (T,C,H,W) -> channel-first required by the model\n",
    "frames = torch.tensor(sequence.copy(), dtype=torch.float,\n",
    "                      device=device).permute(0, 3, 1, 2)\n",
    "original_size = frames.shape[2:]\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# prepare the queries for the cotracker model\n",
    "queries, index = prepare_queries(initial_mask,\n",
    "    fixed_contour_count=50,\n",
    "    device=device,\n",
    "    bulk_grid_size=21,\n",
    "    bulk_margin=0.5,\n",
    "    cotracking_grid_size=21,\n",
    "    original_size=initial_mask.shape)\n",
    "# BxNxC where B=1, C=3, N=number of queries\n",
    "\n",
    "with torch.inference_mode(): # run in inference_mode, as we don't need gradients\n",
    "    original_size = (H, W)\n",
    "    \n",
    "    # setup cotracker\n",
    "    cotracker_predictor(video_chunk=frames.unsqueeze(0), is_first_step=True, queries=queries)\n",
    "\n",
    "    # run cotracker on the whole sequence\n",
    "    step = cotracker_predictor.step\n",
    "    WINDOW = 2 * step\n",
    "    for i in range(0, frames.shape[0] - step, step):\n",
    "        pred_tracks, raw_pred_visibility = cotracker_predictor(\n",
    "            video_chunk=frames[i : i + WINDOW].unsqueeze(0), iters=6\n",
    "        )\n",
    "    \n",
    "    # the predicted visibility is ignored, as mri has no occlusions the way real world videos have\n",
    "    #visibility_threshold = 0.0\n",
    "    #pred_visibility = (raw_pred_visibility > visibility_threshold) if visibility_threshold>0 else torch.ones_like(raw_pred_visibility, dtype=torch.bool)\n",
    "    \n",
    "    cotracker_masks = torch.zeros((T, H, W), dtype=torch.bool, device=device)\n",
    "    for i in range(0, frames.shape[0]):\n",
    "        cotracker_masks[i] = torch.tensor(\n",
    "            points_to_mask(pred_tracks[0,i,:index[\"contour\"][1]].cpu().numpy(), shape=(H,W)), \n",
    "            device=device)\n",
    "        \n",
    "    results = {\n",
    "        \"masks\": cotracker_masks,\n",
    "        # other results that might be useful but are not used in this example\n",
    "        #\"prompts\": pred_tracks[:,:,:index[\"contour\"][1]],\n",
    "        #\"cotracking_prompts\": pred_tracks[:,:,index[\"contour\"][1]:],\n",
    "        #\"pred_visibility\": pred_visibility,\n",
    "        #\"raw_pred_visibility\": raw_pred_visibility\n",
    "    }\n",
    "    # prepare results for SAM mask refinement\n",
    "    results[\"scores\"] = torch.zeros(cotracker_masks.shape[0], dtype=torch.float32, device=device)\n",
    "    results[\"logits\"] = torch.zeros((cotracker_masks.shape[0], 256, 256), dtype=torch.float32, device=device)\n",
    "    \n",
    "    -10 -1 \n",
    "    # log(p_i)\n",
    "    \n",
    "    # initialize sam_mask_threshold with 0.0\n",
    "    sam_mask_threshold = 0.0\n",
    "    \n",
    "    # refine the masks with sam\n",
    "    for i in range(0, frames.shape[0]):\n",
    "        mask, score, logits = refine_with_sam(cotracker_masks[i].detach().clone(),\n",
    "                                             sam_predictor, \n",
    "                                             frames[i].detach().clone(), \n",
    "                                             original_size=(H,W), \n",
    "                                             sam_pass_box_margin=0.5, \n",
    "                                             mask_threshold=sam_mask_threshold,\n",
    "                                             embedding=None)\n",
    "        \n",
    "        \n",
    "        if i == 0: # apply dynamic thresholding on first frame\n",
    "            # set mask threshold to the best threshold\n",
    "            sam_mask_threshold = dynamic_thresholding(initial_mask, logits)\n",
    "\n",
    "        results[\"scores\"][i] = score[0,-1]\n",
    "        results[\"logits\"][i] = logits[0]\n",
    "        results[\"masks\"][i] = mask\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the first mask\n",
    "fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "axs[0].imshow(sequence[0])\n",
    "axs[1].imshow(results[\"masks\"][0].cpu().numpy(), alpha=0.5)\n",
    "axs[0].set_title(\"image\")\n",
    "axs[1].set_title(\"predicted mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example evaluation we compute the bbox center position of the predicted masks\n",
    "# and compare it to the ground truth path\n",
    "\n",
    "bounding_boxes = np.array([compute_bounding_box(\n",
    "    mask.cpu().numpy()) for mask in results[\"masks\"]])\n",
    "box_centers = np.stack((bounding_boxes[:, [0, 2]].mean(\n",
    "    axis=1), bounding_boxes[:, [1, 3]].mean(axis=1)), axis=1)\n",
    "scaling = np.array([W//2, H//2])\n",
    "box_centers = (box_centers - scaling) / scaling\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "\n",
    "axs[0].plot(path[:, 0], \"-\", label=\"ground truth\")\n",
    "axs[0].plot(box_centers[:, 0], \"--\", c=\"red\", label=\"model\")\n",
    "axs[1].plot(path[:, 1], \"-\", label=\"ground truth\")\n",
    "axs[1].plot(box_centers[:, 1], \"--\", c=\"red\", label=\"model\")\n",
    "\n",
    "\n",
    "axs[0].set_xlabel('frame')\n",
    "axs[1].set_xlabel('frame')\n",
    "axs[0].set_ylabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[0].set_ylim(-1, 1)\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up before running the SAM2 based method\n",
    "del sam_predictor\n",
    "del cotracker_predictor\n",
    "torch.cuda.empty_cache()\n",
    "del frames\n",
    "del queries\n",
    "del results\n",
    "del cotracker_masks\n",
    "del pred_tracks\n",
    "del raw_pred_visibility\n",
    "del box_centers\n",
    "del bounding_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SAM2\n",
    "\n",
    "As an alternative model, we implemented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"  # sam2 does not work on cpu only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset frames and initial_mask variables\n",
    "frames = sequence.copy()  # TxHxWxC where C=3 (rgb)\n",
    "initial_mask = initial_mask  # HxW\n",
    "initial_mask.shape, frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sam2 as an alternative foundationmodel\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "checkpoint = \"./checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"  # see sam2/sam2_configs/ for more configs\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, checkpoint, device=\"cuda\")\n",
    "# disable hole filling as is is requires nvcc compilation\n",
    "predictor.fill_hole_area = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAM2 expects a video directory with JPEG frames\n",
    "# This is accomplished by saving the frames temporarily to disk\n",
    "\n",
    "folder_path = \"./.temp\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "for i in range(0, len(frames)):\n",
    "    image = frames[i]\n",
    "    jpg_file_path = os.path.join(folder_path, f\"{i:06d}.jpg\") # 6 digit names 000000.jpg, 000001.jpg, ...\n",
    "    # 80 quality is a good trade-off between file size and quality\n",
    "    cv2.imwrite(jpg_file_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n",
    "\n",
    "# check if the frames are written to disk\n",
    "!ls {folder_path} | head -n 3\n",
    "print(\"...\")\n",
    "print(\"--- Total frames ---\")\n",
    "!ls {folder_path} | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = {\n",
    "    \"logits\": [],\n",
    "    \"masks\": [],\n",
    "    \"scores\": []\n",
    "}\n",
    "with torch.inference_mode(): # again we are using inference_mode\n",
    "    # initialize the state\n",
    "    inference_state = predictor.init_state(video_path=folder_path)\n",
    "\n",
    "    # add the initial mask to the state\n",
    "    frame_idx, obj_ids, video_res_masks = predictor.add_new_mask(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=0, obj_id=1,\n",
    "            mask=initial_mask)\n",
    "    \n",
    "    sam_mask_threshold = None\n",
    "    # propagate the mask through the video\n",
    "    mask_iterator = predictor.propagate_in_video(inference_state, yield_scores=True)\n",
    "    for frame_idx, obj_ids, video_res_masks, scores in mask_iterator:\n",
    "        \n",
    "        if sam_mask_threshold == None: # first pass -> dynamic thresholding\n",
    "            # set mask threshold to the best threshold\n",
    "            sam_mask_threshold = dynamic_thresholding(initial_mask, logits)\n",
    "\n",
    "        results[\"logits\"].append(video_res_masks)\n",
    "        results[\"masks\"].append(video_res_masks > sam_mask_threshold)\n",
    "        results[\"scores\"].append(scores[0,-1])\n",
    "\n",
    "results[\"logits\"] = torch.stack(results[\"logits\"]).squeeze(1,2)\n",
    "results[\"masks\"] = torch.stack(results[\"masks\"]).squeeze(1,2)\n",
    "results[\"scores\"] = torch.tensor(results[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now clean up the temporary directory\n",
    "!rm -r {folder_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview of the first mask\n",
    "fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "axs[0].imshow(sequence[0])\n",
    "axs[1].imshow(results[\"masks\"][0].cpu().numpy(), alpha=0.5)\n",
    "axs[0].set_title(\"image\")\n",
    "axs[1].set_title(\"predicted mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as an example evaluation we compute the bbox center position of the predicted masks and compare it to the ground truth path\n",
    "bounding_boxes = np.array([compute_bounding_box(\n",
    "    mask.cpu().numpy()) for mask in results[\"masks\"]])\n",
    "box_centers = np.stack((bounding_boxes[:, [0, 2]].mean(\n",
    "    axis=1), bounding_boxes[:, [1, 3]].mean(axis=1)), axis=1)\n",
    "scaling = np.array([W//2, H//2])\n",
    "box_centers = (box_centers - scaling) / scaling\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "\n",
    "axs[0].plot(path[:, 0], \"-\", label=\"ground truth\")\n",
    "axs[0].plot(box_centers[:, 0], \"--\", c=\"red\", label=\"model\")\n",
    "axs[1].plot(path[:, 1], \"-\", label=\"ground truth\")\n",
    "axs[1].plot(box_centers[:, 1], \"--\", c=\"red\", label=\"model\")\n",
    "\n",
    "\n",
    "axs[0].set_xlabel('frame')\n",
    "axs[1].set_xlabel('frame')\n",
    "axs[0].set_ylabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[0].set_ylim(-1, 1)\n",
    "axs[0].legend()\n",
    "axs[1].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
